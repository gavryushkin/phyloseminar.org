Heterogeneity in amino acid substitution is an inherent feature of most phylogenomic-scale datasets, and modeling such heterogeneity is now widely seen as important for phylogenomic inference. Site-heterogeneous substitution models such as CAT-F81 and CAT-GTR, as implemented in PhyloBayes, have been forcefully advocated for use on large datasets because they may reduce long-branch attraction artifacts that could result from not adequately modeling amino acid substitutional heterogeneity. However, site-heterogeneous models arguably became popular not because of a deep appreciation for how well they modeled substitutional heterogeneity, but rather because analyses with CAT models often resulted in trees that matched preconceived notions of animal phylogeny (e.g., sponges as the sister lineage to all other extant animals). Importantly, site-heterogeneous models have not been thoroughly compared to other methods for modeling substitutional heterogeneity such as coarse modeling of heterogeneity with data partitioning coupled with site-homogeneous models such as WAG or LG. Here, I show through analyses of simulated and empirical data that data partitioning often performs as well as, or better than, site-heterogeneous CAT models. In contrast to past claims, I demonstrate that partitioning with site-homogeneous models suppresses long-branch attraction artifacts as well as CAT-GTR and much better than CAT-F81. Analyses with data partitioning and site-homogeneous models can require orders of magnitude less computational time than popular site-heterogeneous models, while still resulting in reasonably accurate trees. Although site-heterogeneous models may describe the amino acid substitutional process much better than data partitioning with site-homogeneous models, current implementations of the most popular site-heterogeneous models do not appear to result in more accurate phylogenetic hypotheses than those inferred with partitioning. Thus, the need to model fine-scale site-heterogeneity in phylogenetic inference is called into question.
